# üöÄ AUTOSCALING ANALYSIS - PROJECT PLAN
**Cu·ªôc thi: DATAFLOW 2026**  
**Ch·ªß ƒë·ªÅ: Autoscaling Analysis cho NASA Web Server Logs**

---

## üìã M·ª§C L·ª§C
1. [T·ªïng quan b√†i to√°n](#1-t·ªïng-quan-b√†i-to√°n)
2. [Ph√¢n t√≠ch d·ªØ li·ªáu](#2-ph√¢n-t√≠ch-d·ªØ-li·ªáu)
3. [Feature Engineering](#3-feature-engineering)
4. [K·∫ø ho·∫°ch th·ª±c hi·ªán](#4-k·∫ø-ho·∫°ch-th·ª±c-hi·ªán)
5. [Ki·∫øn tr√∫c h·ªá th·ªëng](#5-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
6. [Timeline & Checklist](#6-timeline--checklist)

---

## 1. T·ªîNG QUAN B√ÄI TO√ÅN

### 1.1 B·ªëi c·∫£nh
Trong qu·∫£n tr·ªã h·ªá th·ªëng ƒë√°m m√¢y, vi·ªác c·∫•p ph√°t t√†i nguy√™n c·ªë ƒë·ªãnh d·∫´n ƒë·∫øn:
- ‚ùå **L√£ng ph√≠ t√†i nguy√™n** khi √≠t ng∆∞·ªùi truy c·∫≠p
- ‚ùå **S·∫≠p h·ªá th·ªëng** khi l∆∞·ª£ng truy c·∫≠p tƒÉng ƒë·ªôt bi·∫øn

### 1.2 M·ª•c ti√™u
X√¢y d·ª±ng h·ªá th·ªëng ph√¢n t√≠ch nh·∫≠t k√Ω truy c·∫≠p ƒë·ªÉ:

| M·ª•c ti√™u | M√¥ t·∫£ |
|----------|-------|
| **B√†i to√°n H·ªìi quy** | D·ª± b√°o l∆∞u l∆∞·ª£ng truy c·∫≠p (s·ªë request, bytes) trong t∆∞∆°ng lai |
| **B√†i to√°n T·ªëi ∆∞u** | T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng server (Autoscaling) ƒë·ªÉ t·ªëi ∆∞u chi ph√≠ |

### 1.3 D·ªØ li·ªáu
- **Ngu·ªìn**: NASA Kennedy Space Center WWW Server Logs
- **Th·ªùi gian**: Th√°ng 7-8/1995 (62 ng√†y)
- **Quy m√¥**: 3.46 tri·ªáu requests (~359 MB)

#### Train/Test Split
| T·∫≠p d·ªØ li·ªáu | Kho·∫£ng th·ªùi gian | S·ªë l∆∞·ª£ng |
|-------------|------------------|----------|
| **Train** | Jul 1 - Aug 22 (53 ng√†y) | 2,934,961 records |
| **Test** | Aug 23 - Aug 31 (9 ng√†y) | 526,651 records |

#### L∆∞u √Ω ƒë·∫∑c bi·ªát
- ‚ö†Ô∏è **Missing data**: Aug 1 14:52 - Aug 3 04:36 (do b√£o)
- ‚úÖ **Parse rate**: 100% (kh√¥ng c√≥ log b·ªã l·ªói format)

### 1.4 Deliverables
| Lo·∫°i | Y√™u c·∫ßu |
|------|---------|
| **M√¥ h√¨nh ML** | T·ªëi thi·ªÉu 2 m√¥ h√¨nh (ARIMA/Prophet/LSTM/XGBoost) |
| **Metrics** | RMSE, MSE, MAE, MAPE |
| **Khung th·ªùi gian** | 1m, 5m, 15m aggregation |
| **Scaling Policy** | Logic rules + cost analysis |
| **Demo** | API (FastAPI) + Dashboard (Streamlit) |
| **Documentation** | B√°o c√°o (max 30 trang), README, slides |
| **Video** | 3-5 ph√∫t demo |

---

## 2. PH√ÇN T√çCH D·ªÆ LI·ªÜU

### 2.1 Th·ªëng k√™ t·ªïng quan

```
üìä DATASET OVERVIEW
‚îú‚îÄ‚îÄ Train records: 2,934,961
‚îú‚îÄ‚îÄ Test records: 526,651
‚îú‚îÄ‚îÄ Total records: 3,461,612
‚îú‚îÄ‚îÄ Unique hosts: ~19,000+
‚îú‚îÄ‚îÄ Date range: Jul 1, 1995 - Aug 31, 1995
‚îî‚îÄ‚îÄ Parse success rate: 100%
```

### 2.2 C·∫•u tr√∫c Log (Apache Combined Format)

```
Format: <host> - - [<timestamp>] "<request>" <status> <bytes>
Example: 199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] "GET /history/apollo/ HTTP/1.0" 200 6245
```

#### Raw Fields
| Field | V√≠ d·ª• | Type | M√¥ t·∫£ |
|-------|-------|------|-------|
| `host` | `199.72.81.55` | String | IP/domain c·ªßa client |
| `timestamp` | `01/Jul/1995:00:00:01 -0400` | Datetime | Th·ªùi ƒëi·ªÉm request |
| `method` | `GET` | Categorical | HTTP method |
| `url` | `/history/apollo/` | String | ƒê∆∞·ªùng d·∫´n resource |
| `protocol` | `HTTP/1.0` | String | Giao th·ª©c |
| `status` | `200` | Integer | M√£ ph·∫£n h·ªìi HTTP |
| `bytes` | `6245` | Integer | Dung l∆∞·ª£ng response |

### 2.3 Ph√¢n b·ªë th·ªùi gian

#### üìÖ Hourly Pattern (EST timezone)
```
00:00 | ################### 950      (Low traffic)
04:00 | ######### 497                (Lowest - 4AM)
08:00 | ######################### 1257
12:00 | ###################################### 1936  (Peak - Noon)
16:00 | ##################################### 1854   (Peak continues)
20:00 | ##################### 1094
23:00 | #################### 1047
```

**Insights:**
- üî¥ Peak hours: 11:00-17:00 EST (gi·ªù l√†m vi·ªác t·∫°i M·ªπ)
- üîµ Low hours: 03:00-06:00 EST
- üìà Pattern: Business hours spike (r√µ r√†ng cho weekly seasonality)

#### üìÜ Daily Trends
```
Jul 1-7   : Stable (~600-900 req/sample)
Jul 8-9   : Drop (~350) - Weekend
Jul 13    : SPIKE (1342) - S·ª± ki·ªán ƒë·∫∑c bi·ªát?
Jul 15-16 : Low (~450) - Weekend
Jul 22-23 : Low (~350) - Weekend
Aug 1-3   : Missing data (Hurricane)
```

**Insights:**
- üìä Weekly seasonality r√µ r√†ng
- üèñÔ∏è Weekend drop ~50%
- ‚ö° Spike detection: Jul 13 (c·∫ßn investigate)
- ‚ö†Ô∏è Missing data handling: Imputation vs. exclusion

#### Top Traffic Spikes
| Timestamp | Estimated Requests/min |
|-----------|------------------------|
| 1995-07-13 09:10 | ~400 |
| 1995-07-13 09:13 | ~400 |
| 1995-07-13 09:49 | ~400 |
| 1995-07-13 08:25-08:46 | ~300 (sustained) |

### 2.4 HTTP Status Codes

| Code | Count | % | Meaning |
|------|-------|---|---------|
| 200 | 9013 | 90.13% | Success |
| 304 | 531 | 5.31% | Not Modified (cache hit) |
| 302 | 408 | 4.08% | Redirect |
| 404 | 48 | 0.48% | Not Found |

**Insights:**
- ‚úÖ Error rate r·∫•t th·∫•p (< 0.5%)
- üíæ Cache efficiency t·ªët (5.3% 304 responses)

### 2.5 Content Analysis

#### HTTP Methods
```
GET:  99.85%
HEAD: 0.14%
POST: 0.01%
```

#### URL Categories
| Category | % | Examples |
|----------|---|----------|
| Images | 33.6% | `/images/*.gif` |
| Shuttle | 33.0% | `/shuttle/missions/*` |
| History | 15.0% | `/history/apollo/*` |
| Other | 13.5% | Root, misc |
| Software | 1.8% | `/software/winvn/*` |
| CGI-bin | 1.7% | `/cgi-bin/imagemap/*` |

#### Content Types (by extension)
| Type | % | Impact |
|------|---|--------|
| GIF images | 56.5% | High bandwidth |
| HTML | 22.4% | Medium bandwidth |
| JPEG | 2.6% | High bandwidth |
| Videos (MPG) | 1.2% | Very high bandwidth |

### 2.6 Response Size Statistics

```
üì¶ SIZE DISTRIBUTION
‚îú‚îÄ‚îÄ Mean:     18 KB
‚îú‚îÄ‚îÄ Median:   3.6 KB
‚îú‚îÄ‚îÄ Std Dev:  64 KB
‚îú‚îÄ‚îÄ Max:      1.2 MB
‚îî‚îÄ‚îÄ Min:      0 bytes (7.6% empty responses)

BUCKETS:
‚îú‚îÄ‚îÄ 0 bytes (empty):  7.6%
‚îú‚îÄ‚îÄ < 1 KB:          22.5%
‚îú‚îÄ‚îÄ 1-10 KB:         44.4%  ‚Üê Most common
‚îú‚îÄ‚îÄ 10-100 KB:       22.5%
‚îú‚îÄ‚îÄ 100KB-1MB:        3.0%
‚îî‚îÄ‚îÄ > 1 MB:           0.1%
```

### 2.7 Bytes Traffic Pattern by Hour

```
00:00 | # 18.0 MB
08:00 | ## 20.7 MB
12:00 | ### 34.7 MB  (Peak)
16:00 | ### 33.7 MB
17:00 | ### 35.1 MB  (Peak)
23:00 | ## 21.1 MB
```

**Insights:**
- üíæ Peak bandwidth: 12:00-17:00
- üìä Bandwidth pattern t∆∞∆°ng ƒë·ªìng request pattern
- ‚ö° Scaling c·∫ßn xem x√©t c·∫£ request count v√† bytes

---

## 3. FEATURE ENGINEERING

### 3.1 Level 1: Time Features (B·∫Øt bu·ªôc)

| Feature | Formula | Type | Purpose |
|---------|---------|------|---------|
| `timestamp` | Original | Datetime | Index |
| `year` | Extract from timestamp | Integer | Long-term trend |
| `month` | Extract from timestamp | Integer | Monthly seasonality |
| `day` | Extract from timestamp | Integer | Daily trend |
| `hour` | Extract from timestamp | Integer [0-23] | Hourly pattern |
| `minute` | Extract from timestamp | Integer [0-59] | Intra-hour pattern |
| `day_of_week` | Monday=0, Sunday=6 | Integer [0-6] | Weekly seasonality |
| `is_weekend` | 1 if Sat/Sun else 0 | Binary | Weekend effect |
| `is_business_hour` | 1 if 8-18h else 0 | Binary | Office hours |
| `time_of_day` | morning/afternoon/evening/night | Categorical | Day segment |

**Code example:**
```python
df['hour'] = df['timestamp'].dt.hour
df['day_of_week'] = df['timestamp'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
df['is_business_hour'] = df['hour'].between(8, 18).astype(int)
```

### 3.2 Level 2: Aggregation Features (Core)

Aggregate log entries theo khung th·ªùi gian:

| Aggregation | Window | Metrics |
|-------------|--------|---------|
| **1 minute** | 1m | `request_count_1m`, `bytes_sum_1m`, `unique_hosts_1m` |
| **5 minutes** | 5m | `request_count_5m`, `bytes_sum_5m`, `error_rate_5m` |
| **15 minutes** | 15m | `request_count_15m`, `bytes_sum_15m`, `avg_bytes_15m` |

**Aggregation metrics:**
```python
agg_funcs = {
    'host': 'nunique',           # unique_hosts
    'bytes': ['sum', 'mean'],    # total & average bandwidth
    'status': [
        ('error_count', lambda x: (x >= 400).sum()),
        ('success_rate', lambda x: (x == 200).mean())
    ]
}
```

### 3.3 Level 3: Lag Features (Time Series)

| Feature | Description | Formula |
|---------|-------------|---------|
| `requests_lag_1` | Previous period | `shift(1)` |
| `requests_lag_5` | 5 periods ago | `shift(5)` |
| `requests_lag_12` | 12 periods ago (1h for 5m) | `shift(12)` |
| `requests_lag_60` | 1 hour ago (for 1m data) | `shift(60)` |
| `requests_lag_288` | Same time yesterday (for 5m) | `shift(288)` |
| `requests_lag_2016` | Same time last week (for 5m) | `shift(2016)` |

**Code example:**
```python
for lag in [1, 5, 12, 60, 288, 2016]:
    df[f'requests_lag_{lag}'] = df['request_count'].shift(lag)
```

### 3.4 Level 4: Rolling Statistics

| Feature | Window | Description |
|---------|--------|-------------|
| `requests_rolling_mean_5` | 5 periods | Short-term average |
| `requests_rolling_mean_15` | 15 periods | Medium-term average |
| `requests_rolling_mean_60` | 60 periods | Long-term average |
| `requests_rolling_std_15` | 15 periods | Volatility measure |
| `requests_rolling_max_30` | 30 periods | Peak detection |
| `requests_rolling_min_30` | 30 periods | Trough detection |
| `bytes_rolling_mean_15` | 15 periods | Bandwidth trend |

**Code example:**
```python
df['requests_rolling_mean_5'] = df['request_count'].rolling(window=5).mean()
df['requests_rolling_std_15'] = df['request_count'].rolling(window=15).std()
```

### 3.5 Level 5: Advanced Features (ƒêi·ªÉm c·ªông)

| Feature | Formula | Purpose |
|---------|---------|---------|
| `requests_diff` | `requests(t) - requests(t-1)` | Trend direction |
| `requests_pct_change` | `(requests(t) - requests(t-1)) / requests(t-1)` | Growth rate |
| `spike_score` | `(x - rolling_mean) / rolling_std` | Anomaly detection |
| `host_entropy` | `-Œ£(p_i * log(p_i))` | DDoS detection |
| `error_burst` | Consecutive errors | Overload indicator |
| `content_ratio` | HTML / (HTML + Images) | Traffic composition |
| `avg_response_size` | `total_bytes / request_count` | Size per request |

**Spike detection:**
```python
df['spike_score'] = (
    (df['request_count'] - df['requests_rolling_mean_15']) 
    / df['requests_rolling_std_15']
)
df['is_spike'] = (df['spike_score'] > 3).astype(int)  # 3-sigma rule
```

**Host entropy (DDoS detection):**
```python
from scipy.stats import entropy

def calculate_host_entropy(hosts):
    """High entropy = many different hosts (normal)
       Low entropy = few hosts dominating (possible DDoS)"""
    value_counts = hosts.value_counts()
    probabilities = value_counts / value_counts.sum()
    return entropy(probabilities)
```

### 3.6 Feature Matrix Summary

| Level | Features Count | Usage |
|-------|----------------|-------|
| Time features | 10 | All models |
| Aggregations | 9 (per window) | Target variables |
| Lag features | 6+ | ARIMA, LSTM |
| Rolling stats | 7+ | Tree-based, Neural Nets |
| Advanced | 7+ | Bonus points |
| **Total** | **~40-50 features** | Final dataset |

### 3.7 Target Variables

| Variable | Description | Use Case |
|----------|-------------|----------|
| `request_count` | Number of requests | Primary target for autoscaling |
| `bytes_sum` | Total bandwidth | Secondary target |
| `unique_hosts` | Active users | Capacity planning |
| `error_rate` | % of errors | Health monitoring |

---

## 4. K·∫æ HO·∫†CH TH·ª∞C HI·ªÜN

### 4.1 Phase 1: Data Pipeline (3 ng√†y)

#### Day 1: Data Ingestion & Cleaning
**File: `notebooks/01_data_ingestion.ipynb`**

```python
# Tasks
1. Load train.txt and test.txt
2. Parse log v·ªõi regex pattern
3. Extract 7 fields (host, timestamp, method, url, protocol, status, bytes)
4. Handle missing data (Aug 1-3 gap)
5. Data quality checks
   - Check parse success rate
   - Validate timestamp continuity
   - Identify outliers in bytes field
6. Save cleaned data ‚Üí data/processed/cleaned_train.parquet
```

**Expected output:**
- `cleaned_train.parquet`: 2.9M rows √ó 7 columns
- `cleaned_test.parquet`: 526K rows √ó 7 columns
- Parse success rate: 100%

#### Day 2: Time Aggregation
**File: `notebooks/02_aggregation.ipynb`**

```python
# Tasks
1. Resample to 1-minute intervals
   - request_count_1m
   - bytes_sum_1m
   - unique_hosts_1m
   - error_rate_1m
   
2. Resample to 5-minute intervals
   - request_count_5m
   - bytes_sum_5m
   - avg_response_size_5m
   
3. Resample to 15-minute intervals
   - request_count_15m
   - bytes_sum_15m
   
4. Handle missing periods (fill with 0 or interpolate)
5. Save aggregated data
```

**Expected output:**
- `train_1m.parquet`: ~76,000 rows
- `train_5m.parquet`: ~15,200 rows
- `train_15m.parquet`: ~5,100 rows

#### Day 3: Feature Engineering
**File: `notebooks/03_feature_engineering.ipynb`**

```python
# Tasks
1. Add time features (hour, day_of_week, is_weekend, etc.)
2. Create lag features (1, 5, 12, 60, 288, 2016)
3. Calculate rolling statistics (mean, std, max, min)
4. Advanced features (spike_score, host_entropy)
5. Handle NaN values from lag/rolling (forward fill or drop)
6. Train/test split based on date
7. Save final feature sets
```

**Expected output:**
- `train_features_1m.parquet`: ~40 features
- `train_features_5m.parquet`: ~40 features
- `train_features_15m.parquet`: ~40 features
- Feature importance analysis

---

### 4.2 Phase 2: EDA & Insights (2 ng√†y)

#### Day 4: Exploratory Data Analysis
**File: `notebooks/04_eda.ipynb`**

**Visualizations to create:**

1. **Time Series Plots**
   ```python
   - Line plot: Daily request volume
   - Line plot: Hourly pattern (average by hour)
   - Heatmap: Day of week √ó Hour
   ```

2. **Seasonality Analysis**
   ```python
   from statsmodels.tsa.seasonal import seasonal_decompose
   - Decompose: Trend + Seasonal + Residual
   - ACF/PACF plots for ARIMA order selection
   ```

3. **Distribution Analysis**
   ```python
   - Histogram: Request counts
   - Box plot: Request by hour
   - Violin plot: Request by day of week
   ```

4. **Correlation Analysis**
   ```python
   - Correlation matrix heatmap
   - Feature importance (preliminary)
   ```

5. **Anomaly Detection Visual**
   ```python
   - Highlight spikes (Jul 13)
   - Mark missing data period (Aug 1-3)
   ```

**Expected output:**
- 15-20 visualizations
- Key insights document
- Recommended features for modeling

#### Day 5: Hypothesis & Feature Selection
**File: `notebooks/05_feature_selection.ipynb`**

```python
# Tasks
1. Test hypotheses:
   - Weekend traffic is significantly lower? (t-test)
   - Business hours have higher traffic? (ANOVA)
   - Is there weekly seasonality? (FFT analysis)
   
2. Feature selection:
   - Remove highly correlated features (> 0.95)
   - Feature importance from RandomForest
   - Mutual information scores
   
3. Prepare final feature sets for each model type
```

**Expected output:**
- Selected features list (25-30 features)
- Statistical test results
- Feature engineering insights

---

### 4.3 Phase 3: Modeling (4 ng√†y)

#### Day 6-7: Baseline Models
**File: `notebooks/06_baseline_models.ipynb`**

**Model 1: SARIMA**
```python
from statsmodels.tsa.statespace.sarimax import SARIMAX

# Tasks
1. ACF/PACF analysis for order selection
2. Grid search for (p,d,q)(P,D,Q,m) parameters
3. Fit on train data (1m, 5m, 15m windows)
4. Forecast on test set
5. Evaluate: RMSE, MAE, MAPE
6. Plot: Predicted vs Actual
```

**Expected metrics (example):**
- RMSE (1m): ~15-20 requests
- RMSE (5m): ~50-80 requests
- RMSE (15m): ~150-250 requests

**Model 2: Prophet**
```python
from prophet import Prophet

# Tasks
1. Prepare data format (ds, y)
2. Add seasonality components:
   - Yearly: False (only 2 months data)
   - Weekly: True
   - Daily: True
3. Add holidays/events (Jul 13 spike?)
4. Fit and forecast
5. Evaluate metrics
6. Component plots
```

**Expected metrics:**
- RMSE (5m): ~40-60 requests
- MAPE: 15-25%

#### Day 8-9: Advanced Models
**File: `notebooks/07_ml_models.ipynb`**

**Time Series Cross-Validation Strategy:**
```python
from sklearn.model_selection import TimeSeriesSplit

# CRITICAL: Use expanding window CV (no data leakage)
tscv = TimeSeriesSplit(n_splits=5, gap=12)  # 12-period gap to prevent leakage

# Validation splits visualization:
# Fold 1: [Train: Day 1-10 ] | Gap | [Val: Day 11-15]
# Fold 2: [Train: Day 1-15 ] | Gap | [Val: Day 16-20]
# Fold 3: [Train: Day 1-20 ] | Gap | [Val: Day 21-25]
# ...
```

**Model 3: LightGBM**
```python
import lightgbm as lgb
import optuna

# Tasks
1. Prepare feature matrix (X) and target (y)
2. Time-based split validation (TimeSeriesSplit with gap)
3. Hyperparameter tuning with Optuna:
   - num_leaves: [31, 63, 127, 255]
   - learning_rate: [0.01, 0.03, 0.05, 0.1]
   - n_estimators: [100, 300, 500, 1000]
   - min_child_samples: [5, 10, 20]
   - subsample: [0.6, 0.8, 1.0]
   - colsample_bytree: [0.6, 0.8, 1.0]
4. Train on all features with early stopping
5. Feature importance analysis (SHAP values)
6. Evaluate on test set
7. Log experiments to MLflow
```

**Expected metrics:**
- RMSE (5m): ~30-45 requests (best)
- Feature importance: lag features + rolling stats top

**Model 4 (Optional): LSTM**
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Tasks
1. Reshape data for LSTM (samples, timesteps, features)
2. Architecture:
   - LSTM(64) ‚Üí Dropout(0.2) ‚Üí LSTM(32) ‚Üí Dense(1)
3. Train with early stopping
4. Evaluate on test set
5. Visualize learning curves
```

**Expected metrics:**
- RMSE (5m): ~35-50 requests
- Training time: ~30-60 mins

#### Model Comparison Matrix

| Model | RMSE (5m) | MAE (5m) | MAPE | Training Time | Pros | Cons |
|-------|-----------|----------|------|---------------|------|------|
| SARIMA | ~60 | ~45 | 20% | 5 min | Interpretable, statistical | Slow, limited features |
| Prophet | ~50 | ~38 | 18% | 2 min | Easy, seasonality | Black box |
| LightGBM | ~35 | ~25 | 12% | 1 min | Fast, accurate | Needs features |
| LSTM | ~40 | ~30 | 15% | 45 min | Sequences | Slow, overfitting risk |

**Recommendation: LightGBM for production**

---

### 4.4 Phase 4: Autoscaling Logic (3 ng√†y)

#### Day 10: Scaling Policy Design
**File: `notebooks/08_scaling_policy.ipynb`**

**Scaling Parameters:**
```python
class ScalingConfig:
    # Capacity
    MIN_SERVERS = 1
    MAX_SERVERS = 20
    REQUESTS_PER_SERVER_PER_MIN = 100  # Capacity threshold
    
    # Thresholds
    SCALE_OUT_THRESHOLD = 0.80  # 80% capacity ‚Üí add server
    SCALE_IN_THRESHOLD = 0.30   # 30% capacity ‚Üí remove server
    
    # Timing
    SCALE_OUT_CONSECUTIVE = 5   # 5 minutes sustained high load
    SCALE_IN_CONSECUTIVE = 10   # 10 minutes sustained low load
    COOLDOWN_MINUTES = 5        # Wait 5 min between scaling actions
    
    # Hysteresis
    SCALE_OUT_INCREMENT = 2     # Add 2 servers at once
    SCALE_IN_DECREMENT = 1      # Remove 1 server at a time
```

**Scaling Algorithm:**
```python
def recommend_scaling(predicted_load, current_servers, history):
    """
    Args:
        predicted_load: List of predicted requests for next N minutes
        current_servers: Current number of active servers
        history: Recent scaling actions
    
    Returns:
        action: 'scale_out', 'scale_in', or 'hold'
        target_servers: Recommended server count
    """
    # 1. Calculate required servers
    required_servers = ceil(predicted_load.mean() / REQUESTS_PER_SERVER_PER_MIN)
    
    # 2. Calculate current utilization
    utilization = predicted_load.mean() / (current_servers * REQUESTS_PER_SERVER_PER_MIN)
    
    # 3. Check cooldown
    if last_action_time < COOLDOWN_MINUTES:
        return 'hold', current_servers
    
    # 4. Scale out logic
    if utilization > SCALE_OUT_THRESHOLD:
        consecutive_high = count_consecutive_high(predicted_load)
        if consecutive_high >= SCALE_OUT_CONSECUTIVE:
            target = min(current_servers + SCALE_OUT_INCREMENT, MAX_SERVERS)
            return 'scale_out', target
    
    # 5. Scale in logic
    elif utilization < SCALE_IN_THRESHOLD:
        consecutive_low = count_consecutive_low(predicted_load)
        if consecutive_low >= SCALE_IN_CONSECUTIVE:
            target = max(current_servers - SCALE_IN_DECREMENT, MIN_SERVERS)
            return 'scale_in', target
    
    # 6. Hold (do nothing)
    return 'hold', current_servers
```

#### Day 11: Simulation & Cost Analysis
**File: `notebooks/09_cost_simulation.ipynb`**

**Simulation:**
```python
# Simulate autoscaling on test set
results = []
current_servers = 1
scaling_events = []

for t, row in test_df.iterrows():
    # 1. Get prediction for next 15 minutes
    predicted = model.predict(row)
    
    # 2. Recommend scaling
    action, target = recommend_scaling(predicted, current_servers, scaling_events)
    
    # 3. Record metrics
    results.append({
        'timestamp': t,
        'actual_load': row['request_count'],
        'predicted_load': predicted,
        'servers': current_servers,
        'utilization': row['request_count'] / (current_servers * 100),
        'action': action
    })
    
    # 4. Execute action
    if action in ['scale_out', 'scale_in']:
        scaling_events.append({'time': t, 'action': action, 'from': current_servers, 'to': target})
        current_servers = target
```

**Cost Analysis:**
```python
# Pricing (v√≠ d·ª•)
COST_PER_SERVER_PER_HOUR = 0.10  # $0.10/server/hour

# Strategy 1: Fixed (always max servers)
fixed_cost = MAX_SERVERS * COST_PER_SERVER_PER_HOUR * total_hours

# Strategy 2: Autoscaling
autoscale_cost = sum(servers_at_time * COST_PER_SERVER_PER_HOUR * (1/60)) for each minute

# Strategy 3: Fixed minimal (always min servers)
minimal_cost = MIN_SERVERS * COST_PER_SERVER_PER_HOUR * total_hours

# Performance metrics
sla_violations = count(utilization > 1.0)  # Overloaded periods
wasted_capacity = count(utilization < 0.3)  # Underutilized periods
```

**Expected results:**
- Autoscaling cost: ~40-60% reduction vs. fixed max
- SLA violations: < 1% of time
- Avg utilization: 60-80%

#### Day 12: Optimization & Tuning
**File: `notebooks/10_policy_optimization.ipynb`**

```python
# Tasks
1. Grid search optimal thresholds:
   - SCALE_OUT_THRESHOLD: [0.70, 0.75, 0.80, 0.85]
   - SCALE_IN_THRESHOLD: [0.20, 0.25, 0.30, 0.35]
   
2. Test different cooldown periods: [3, 5, 7, 10] minutes

3. Evaluate trade-offs:
   - Cost vs. SLA compliance
   - Responsiveness vs. stability (flapping)
   
4. Visualize:
   - Cost-performance frontier
   - Scaling timeline with events
   - Utilization heatmap
```

---

### 4.5 Phase 5: Deployment (3 ng√†y)

#### Day 13-14: API Development
**File: `src/api/main.py`**

```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib

app = FastAPI(title="Autoscaling Prediction API")

# Load models
model_5m = joblib.load('models/lgbm_5m.pkl')
scaler = joblib.load('models/scaler.pkl')

# Endpoints
@app.post("/forecast")
async def forecast(horizon: int = 30, confidence: float = 0.95):
    """Predict request volume for next N minutes with prediction intervals"""
    predictions = model_5m.predict(horizon)
    lower, upper = model_5m.predict_interval(horizon, confidence)
    return {
        "horizon_minutes": horizon,
        "predictions": predictions.tolist(),
        "prediction_lower": lower.tolist(),  # Lower bound
        "prediction_upper": upper.tolist(),  # Upper bound
        "confidence_level": confidence,
        "mean_load": float(predictions.mean()),
        "peak_load": float(predictions.max())
    }

@app.post("/recommend-scaling")
async def recommend_scaling(predicted_load: list, current_servers: int):
    """Get scaling recommendation"""
    action, target = scaling_policy.recommend(predicted_load, current_servers)
    return {
        "action": action,
        "target_servers": target,
        "current_servers": current_servers,
        "estimated_utilization": calculate_utilization(predicted_load, target)
    }

@app.get("/metrics")
async def get_metrics():
    """Current system metrics"""
    return {
        "model_rmse": 35.2,
        "model_mae": 25.1,
        "avg_servers": 3.4,
        "cost_reduction": "45%"
    }

@app.get("/cost-report")
async def cost_report(start_date: str, end_date: str):
    """Cost analysis for date range"""
    return {
        "period": f"{start_date} to {end_date}",
        "total_cost": 123.45,
        "fixed_cost_comparison": 234.56,
        "savings": 111.11,
        "scaling_events": 42
    }
```

**Testing:**
```bash
# Start server
uvicorn src.api.main:app --reload --port 8000

# Test endpoints
curl -X POST "http://localhost:8000/forecast?horizon=30"
curl -X POST "http://localhost:8000/recommend-scaling" \
  -H "Content-Type: application/json" \
  -d '{"predicted_load": [120, 150, 180], "current_servers": 2}'
```

#### Day 15: Dashboard Development
**File: `app/dashboard.py`**

```python
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import requests

st.set_page_config(page_title="Autoscaling Dashboard", layout="wide")

# Sidebar
st.sidebar.title("‚öôÔ∏è Configuration")
time_window = st.sidebar.selectbox("Time Window", ["1 minute", "5 minutes", "15 minutes"])
forecast_horizon = st.sidebar.slider("Forecast Horizon (min)", 5, 60, 30)

# Main dashboard
st.title("üöÄ Autoscaling Analysis Dashboard")

# Row 1: Metrics
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Current Load", "145 req/min", "+12%")
with col2:
    st.metric("Active Servers", "3", "‚Üë1")
with col3:
    st.metric("Utilization", "75%", "+5%")
with col4:
    st.metric("Est. Cost/Hour", "$0.30", "-$0.15")

# Row 2: Time Series
st.subheader("üìà Traffic & Forecast")
col1, col2 = st.columns([2, 1])

with col1:
    # Historical + Forecast plot
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['actual'], name='Actual', mode='lines'))
    fig.add_trace(go.Scatter(x=df_pred['timestamp'], y=df_pred['predicted'], name='Forecast', mode='lines', line=dict(dash='dash')))
    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['servers']*100, name='Capacity', mode='lines', line=dict(color='green')))
    st.plotly_chart(fig, use_container_width=True)

with col2:
    # Scaling recommendation
    st.subheader("üéØ Recommendation")
    prediction = requests.post(f"http://localhost:8000/forecast?horizon={forecast_horizon}").json()
    recommendation = requests.post("http://localhost:8000/recommend-scaling", 
                                   json={"predicted_load": prediction['predictions'], "current_servers": 3}).json()
    
    if recommendation['action'] == 'scale_out':
        st.error(f"‚¨ÜÔ∏è SCALE OUT to {recommendation['target_servers']} servers")
    elif recommendation['action'] == 'scale_in':
        st.info(f"‚¨áÔ∏è SCALE IN to {recommendation['target_servers']} servers")
    else:
        st.success("‚úÖ HOLD - Current capacity adequate")
    
    st.metric("Predicted Avg Load", f"{prediction['mean_load']:.0f} req/min")
    st.metric("Predicted Peak", f"{prediction['peak_load']:.0f} req/min")

# Row 3: Analysis
tab1, tab2, tab3 = st.tabs(["üìä Patterns", "üí∞ Cost Analysis", "‚ö†Ô∏è Anomalies"])

with tab1:
    col1, col2 = st.columns(2)
    with col1:
        # Hourly pattern
        fig = px.box(df, x='hour', y='request_count', title='Request Volume by Hour')
        st.plotly_chart(fig, use_container_width=True)
    with col2:
        # Day of week pattern
        fig = px.box(df, x='day_of_week', y='request_count', title='Request Volume by Day of Week')
        st.plotly_chart(fig, use_container_width=True)

with tab2:
    # Cost comparison
    cost_data = pd.DataFrame({
        'Strategy': ['Fixed (Max)', 'Autoscaling', 'Fixed (Min)'],
        'Cost': [234.56, 123.45, 67.89],
        'SLA Violations': [0, 5, 342]
    })
    fig = px.bar(cost_data, x='Strategy', y='Cost', title='Cost Comparison')
    st.plotly_chart(fig, use_container_width=True)
    
    st.metric("Cost Savings", "$111.11", "-47%")
    st.metric("SLA Compliance", "99.5%", "+0.5%")

with tab3:
    # Anomaly detection
    anomalies = df[df['spike_score'] > 3]
    st.dataframe(anomalies[['timestamp', 'request_count', 'spike_score']])
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df['timestamp'], y=df['request_count'], name='Normal', mode='markers', marker=dict(size=4)))
    fig.add_trace(go.Scatter(x=anomalies['timestamp'], y=anomalies['request_count'], name='Anomaly', mode='markers', marker=dict(size=10, color='red')))
    st.plotly_chart(fig, use_container_width=True)
```

**Running:**
```bash
streamlit run app/dashboard.py
```

---

### 4.6 Phase 6: Documentation (2 ng√†y)

#### Day 16: Report & README
**Files: `reports/report.pdf`, `README.md`**

**Report Structure (max 30 pages):**
1. Executive Summary (1 page)
2. Problem Statement (2 pages)
3. Data Analysis (5 pages)
   - Data overview
   - EDA insights
   - Feature engineering
4. Methodology (8 pages)
   - Model selection rationale
   - Training process
   - Hyperparameter tuning
5. Results (6 pages)
   - Model comparison
   - Performance metrics
   - Scaling policy evaluation
6. Deployment (3 pages)
   - Architecture
   - API documentation
   - Dashboard features
7. Conclusion & Future Work (2 pages)
8. References (1 page)
9. Appendix (2 pages)
   - Code snippets
   - Additional visualizations

**README.md (follow sample-README.md template):**
- Project overview
- Installation instructions
- Usage guide
- API endpoints
- Model performance
- Team info

#### Day 17: Slides & Video
**Files: `reports/slides.pptx`, `demo_video.mp4`**

**Slide Structure (15-20 slides):**
1. Title & Team (1)
2. Problem Introduction (2)
3. Data Overview (2)
4. Key Insights from EDA (3)
5. Modeling Approach (3)
6. Results & Comparison (3)
7. Autoscaling Logic (2)
8. Demo (2)
9. Cost Analysis (1)
10. Conclusion & Q&A (1)

**Video Demo Script (3-5 minutes):**
```
[0:00-0:30] Introduction & Problem
[0:30-1:00] Data visualization highlights
[1:00-2:00] Live API demo
  - Call /forecast endpoint
  - Show prediction results
  - Call /recommend-scaling
[2:00-3:30] Dashboard walkthrough
  - Real-time metrics
  - Forecast visualization
  - Scaling recommendations
  - Cost analysis tab
[3:30-4:00] Key results & benefits
[4:00-4:30] Q&A preparation
```

---

## 5. KI·∫æN TR√öC H·ªÜ TH·ªêNG

### 5.1 Data Flow Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         DATA LAYER                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Raw Logs (train.txt, test.txt)                                 ‚îÇ
‚îÇ       ‚Üì                                                          ‚îÇ
‚îÇ  Parser (regex extraction)                                       ‚îÇ
‚îÇ       ‚Üì                                                          ‚îÇ
‚îÇ  Cleaned Data (Parquet)                                          ‚îÇ
‚îÇ       ‚Üì                                                          ‚îÇ
‚îÇ  Aggregator (1m, 5m, 15m windows)                               ‚îÇ
‚îÇ       ‚Üì                                                          ‚îÇ
‚îÇ  Feature Engineer (40+ features)                                 ‚îÇ
‚îÇ       ‚Üì                                                          ‚îÇ
‚îÇ  Final Datasets (train_features_*.parquet)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       MODELING LAYER                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ SARIMA  ‚îÇ  ‚îÇ Prophet ‚îÇ  ‚îÇ LightGBM ‚îÇ  ‚îÇ LSTM ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                         ‚Üì                                        ‚îÇ
‚îÇ              Model Selection (LightGBM)                          ‚îÇ
‚îÇ                         ‚Üì                                        ‚îÇ
‚îÇ             Saved Model (.pkl, .joblib)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   APPLICATION LAYER                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ   FastAPI        ‚îÇ              ‚îÇ   Streamlit        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   (Backend)      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   (Frontend)       ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ    REST API  ‚îÇ                    ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  /forecast       ‚îÇ              ‚îÇ  üìà Visualizations ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  /recommend      ‚îÇ              ‚îÇ  üéõÔ∏è Controls       ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  /metrics        ‚îÇ              ‚îÇ  üìä Analytics      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  /cost-report    ‚îÇ              ‚îÇ  üí∞ Cost Dashboard ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     SCALING LAYER                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Autoscaling Policy Engine                                       ‚îÇ
‚îÇ    ‚Üì                                                             ‚îÇ
‚îÇ  Simulator (Cost & Performance Analysis)                         ‚îÇ
‚îÇ    ‚Üì                                                             ‚îÇ
‚îÇ  Recommendations (scale_out / scale_in / hold)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 Directory Structure (Final)

```
datafollow/
‚îú‚îÄ‚îÄ DATA/
‚îÇ   ‚îú‚îÄ‚îÄ train.txt                      # Raw log (304 MB)
‚îÇ   ‚îú‚îÄ‚îÄ test.txt                       # Raw log (54 MB)
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaned_train.parquet      # Cleaned raw data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaned_test.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_1m.parquet           # Aggregated 1-min
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_5m.parquet           # Aggregated 5-min
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_15m.parquet          # Aggregated 15-min
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_features_1m.parquet  # Final features
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_features_5m.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_features_15m.parquet
‚îÇ   ‚îî‚îÄ‚îÄ sample-README.md               # Template
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_ingestion.ipynb        # Parse & clean
‚îÇ   ‚îú‚îÄ‚îÄ 02_aggregation.ipynb           # Time windows
‚îÇ   ‚îú‚îÄ‚îÄ 03_feature_engineering.ipynb   # Features
‚îÇ   ‚îú‚îÄ‚îÄ 04_eda.ipynb                   # Visualizations
‚îÇ   ‚îú‚îÄ‚îÄ 05_feature_selection.ipynb     # Feature importance
‚îÇ   ‚îú‚îÄ‚îÄ 06_baseline_models.ipynb       # SARIMA, Prophet
‚îÇ   ‚îú‚îÄ‚îÄ 07_ml_models.ipynb             # LightGBM, LSTM
‚îÇ   ‚îú‚îÄ‚îÄ 08_scaling_policy.ipynb        # Policy design
‚îÇ   ‚îú‚îÄ‚îÄ 09_cost_simulation.ipynb       # Simulation
‚îÇ   ‚îî‚îÄ‚îÄ 10_policy_optimization.ipynb   # Tuning
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.py                  # Log parsing logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py                 # Data cleaning
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aggregator.py              # Time aggregation
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ time_features.py           # Hour, day, etc.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lag_features.py            # Lag generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rolling_features.py        # Rolling stats
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ advanced_features.py       # Spike, entropy
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sarima.py                  # SARIMA wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prophet_model.py           # Prophet wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lgbm_model.py              # LightGBM wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lstm_model.py              # LSTM implementation
‚îÇ   ‚îú‚îÄ‚îÄ scaling/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Scaling parameters
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ policy.py                  # Scaling logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ simulator.py               # Cost simulation
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                 # RMSE, MAE, MAPE
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization.py           # Plot helpers
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ main.py                    # FastAPI app
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py                   # Streamlit dashboard
‚îÇ
‚îú‚îÄ‚îÄ models/                            # Saved models
‚îÇ   ‚îú‚îÄ‚îÄ sarima_5m.pkl
‚îÇ   ‚îú‚îÄ‚îÄ prophet_5m.pkl
‚îÇ   ‚îú‚îÄ‚îÄ lgbm_5m.pkl
‚îÇ   ‚îú‚îÄ‚îÄ lgbm_1m.pkl
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl
‚îÇ   ‚îî‚îÄ‚îÄ feature_names.json
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ report.pdf                     # Final report (max 30 pages)
‚îÇ   ‚îú‚îÄ‚îÄ slides.pptx                    # Presentation
‚îÇ   ‚îú‚îÄ‚îÄ demo_video.mp4                 # 3-5 min video
‚îÇ   ‚îî‚îÄ‚îÄ figures/                       # All visualizations
‚îÇ
‚îú‚îÄ‚îÄ tests/                             # Unit tests (required)
‚îÇ   ‚îú‚îÄ‚îÄ test_parser.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py
‚îÇ   ‚îî‚îÄ‚îÄ test_scaling_policy.py
‚îÇ
‚îú‚îÄ‚îÄ mlruns/                            # MLflow experiment tracking
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci.yml                     # Run tests on PR
‚îÇ       ‚îî‚îÄ‚îÄ cd.yml                     # Deploy pipeline
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .pre-commit-config.yaml            # Pre-commit hooks
‚îú‚îÄ‚îÄ Dockerfile                         # API container
‚îú‚îÄ‚îÄ Dockerfile.streamlit               # Dashboard container
‚îú‚îÄ‚îÄ docker-compose.yml                 # Multi-container setup
‚îú‚îÄ‚îÄ dvc.yaml                           # DVC pipeline
‚îú‚îÄ‚îÄ pyproject.toml                     # Project config (replaces setup.py)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md                          # Main documentation
‚îî‚îÄ‚îÄ PROJECT_PLAN.md                    # This file
```

### 5.3 Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Data Processing** | Pandas, Polars, NumPy | Data manipulation |
| **Time Series** | Statsmodels, Prophet, sktime | SARIMA, forecasting |
| **ML Models** | LightGBM, XGBoost, Scikit-learn | Gradient boosting |
| **Deep Learning** | PyTorch, Lightning | LSTM |
| **Hyperparameter Tuning** | Optuna | Bayesian optimization |
| **Visualization** | Matplotlib, Seaborn, Plotly | Charts & plots |
| **API** | FastAPI | REST endpoints |
| **Dashboard** | Streamlit | Interactive UI |
| **Storage** | Parquet (PyArrow) | Efficient data format |
| **MLOps** | MLflow, DVC | Experiment tracking |
| **Containerization** | Docker, Docker Compose | Deployment |
| **Version Control** | Git | Code management |
| **Code Quality** | Ruff, Black, pre-commit | Linting, formatting |

### 5.3.1 MLOps Pipeline

```
Experiment Tracking (MLflow):
‚îú‚îÄ‚îÄ Log parameters, metrics, artifacts
‚îú‚îÄ‚îÄ Model registry (staging -> production)
‚îî‚îÄ‚îÄ Compare runs across experiments

Data Versioning (DVC):
‚îú‚îÄ‚îÄ Track large data files (train.txt, test.txt)
‚îú‚îÄ‚îÄ Reproducible pipelines
‚îî‚îÄ‚îÄ Remote storage (S3/GCS compatible)
```

### 5.3.2 Docker Configuration

```dockerfile
# Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY src/ ./src/
COPY models/ ./models/
EXPOSE 8000
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8501:8501"
    depends_on:
      - api
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    ports:
      - "5000:5000"
```

### 5.3.3 Pre-commit Configuration

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.2.0
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
        args: ['--maxkb=1000']

  - repo: local
    hooks:
      - id: pytest
        name: pytest
        entry: pytest tests/ -v --tb=short
        language: system
        pass_filenames: false
        always_run: true
```

### 5.4 Dependencies (requirements.txt)

```txt
# Core data processing
pandas>=2.2.0
numpy>=1.26.0
pyarrow>=15.0.0
polars>=0.20.0  # Fast alternative for large data

# Time series & forecasting
statsmodels>=0.14.1
prophet>=1.1.5
sktime>=0.26.0  # Unified time series framework

# Machine learning
scikit-learn>=1.4.0
lightgbm>=4.3.0
xgboost>=2.0.3
optuna>=3.5.0  # Hyperparameter optimization

# Deep learning (optional)
torch>=2.2.0  # PyTorch for LSTM (lighter than TensorFlow)
lightning>=2.2.0  # PyTorch Lightning for training

# Visualization
matplotlib>=3.8.0
seaborn>=0.13.0
plotly>=5.18.0

# Web frameworks
fastapi>=0.109.0
uvicorn[standard]>=0.27.0
streamlit>=1.31.0

# MLOps & Experiment Tracking
mlflow>=2.10.0  # Experiment tracking
dvc>=3.42.0  # Data version control

# Utilities
python-dateutil>=2.8.2
scipy>=1.12.0
joblib>=1.3.2
pydantic>=2.6.0

# Development & Testing
jupyter>=1.0.0
notebook>=7.1.0
ipykernel>=6.29.0
pytest>=8.0.0
pytest-cov>=4.1.0  # Coverage reporting
black>=24.1.0
ruff>=0.2.0  # Fast linter
pre-commit>=3.6.0  # Git hooks

# Containerization
docker>=7.0.0  # Docker SDK
```

---

## 6. TIMELINE & CHECKLIST

### 6.1 Gantt Chart Overview

```
Week 1: Data Pipeline & EDA
‚îú‚îÄ‚îÄ Day 1  : ‚ñ†‚ñ†‚ñ†‚ñ† Data Ingestion + DVC Setup
‚îú‚îÄ‚îÄ Day 2  : ‚ñ†‚ñ†‚ñ†‚ñ† Aggregation
‚îú‚îÄ‚îÄ Day 3  : ‚ñ†‚ñ†‚ñ†‚ñ† Feature Engineering
‚îú‚îÄ‚îÄ Day 4  : ‚ñ†‚ñ†‚ñ†‚ñ† EDA
‚îî‚îÄ‚îÄ Day 5  : ‚ñ†‚ñ†‚ñ†‚ñ† Feature Selection

Week 2: Modeling + MLOps
‚îú‚îÄ‚îÄ Day 6  : ‚ñ†‚ñ†‚ñ†‚ñ† MLflow Setup + SARIMA
‚îú‚îÄ‚îÄ Day 7  : ‚ñ†‚ñ†‚ñ†‚ñ† Prophet
‚îú‚îÄ‚îÄ Day 8  : ‚ñ†‚ñ†‚ñ†‚ñ† LightGBM + Optuna
‚îî‚îÄ‚îÄ Day 9  : ‚ñ†‚ñ†‚ñ†‚ñ† LSTM + Model Comparison

Week 3: Autoscaling & Deployment
‚îú‚îÄ‚îÄ Day 10 : ‚ñ†‚ñ†‚ñ†‚ñ† Scaling Policy
‚îú‚îÄ‚îÄ Day 11 : ‚ñ†‚ñ†‚ñ†‚ñ† Cost Simulation
‚îú‚îÄ‚îÄ Day 12 : ‚ñ†‚ñ†‚ñ†‚ñ† Policy Optimization
‚îú‚îÄ‚îÄ Day 13 : ‚ñ†‚ñ†‚ñ†‚ñ† API Development + Docker
‚îî‚îÄ‚îÄ Day 14 : ‚ñ†‚ñ†‚ñ†‚ñ† Dashboard Development

Week 4: Testing, CI/CD & Documentation
‚îú‚îÄ‚îÄ Day 15 : ‚ñ†‚ñ†‚ñ†‚ñ† Unit Tests + CI/CD Pipeline
‚îú‚îÄ‚îÄ Day 16 : ‚ñ†‚ñ†‚ñ†‚ñ† Integration Tests + Docker Compose
‚îú‚îÄ‚îÄ Day 17 : ‚ñ†‚ñ†‚ñ†‚ñ† Report Writing
‚îú‚îÄ‚îÄ Day 18 : ‚ñ†‚ñ†‚ñ†‚ñ† Slides & Video
‚îî‚îÄ‚îÄ Day 19 : ‚ñ†‚ñ†‚ñ†‚ñ† Final Review & Submission
```

### 6.2 Detailed Checklist

#### ‚úÖ Phase 1: Data Pipeline
- [ ] Load train.txt and test.txt successfully
- [ ] Parse all records with regex (100% success rate)
- [ ] Extract 7 fields correctly
- [ ] Handle missing data period (Aug 1-3)
- [ ] Create 1-minute aggregation
- [ ] Create 5-minute aggregation
- [ ] Create 15-minute aggregation
- [ ] Generate time features (hour, day_of_week, etc.)
- [ ] Create lag features (1, 5, 12, 60, 288, 2016)
- [ ] Calculate rolling statistics (mean, std, max, min)
- [ ] Implement advanced features (spike_score, host_entropy)
- [ ] Save all processed datasets to Parquet

#### ‚úÖ Phase 2: EDA
- [ ] Create time series line plots
- [ ] Generate hourly pattern visualization
- [ ] Create day-of-week heatmap
- [ ] Perform seasonality decomposition
- [ ] Plot ACF/PACF for ARIMA
- [ ] Create correlation matrix
- [ ] Visualize distribution (histogram, box plots)
- [ ] Identify and mark anomalies (Jul 13 spike)
- [ ] Statistical hypothesis tests (weekend effect, business hours)
- [ ] Feature importance preliminary analysis
- [ ] Document key insights

#### ‚úÖ Phase 3: Modeling
- [ ] Set up MLflow experiment tracking
- [ ] Implement TimeSeriesSplit cross-validation with gap
- [ ] Implement SARIMA model
- [ ] Tune SARIMA hyperparameters (p,d,q)(P,D,Q,m)
- [ ] Evaluate SARIMA: RMSE, MAE, MAPE
- [ ] Implement Prophet model
- [ ] Add seasonality components to Prophet
- [ ] Evaluate Prophet metrics
- [ ] Implement LightGBM model
- [ ] Hyperparameter tuning with Optuna (Bayesian)
- [ ] Feature importance analysis (SHAP values)
- [ ] Evaluate LightGBM metrics
- [ ] (Optional) Implement LSTM with PyTorch Lightning
- [ ] Compare all models (create comparison table)
- [ ] Select best model for production
- [ ] Register model in MLflow Model Registry
- [ ] Save trained models with versioning

#### ‚úÖ Phase 4: Autoscaling
- [ ] Define scaling configuration (thresholds, cooldown, etc.)
- [ ] Implement scaling recommendation function
- [ ] Add cooldown mechanism
- [ ] Implement hysteresis logic
- [ ] Simulate autoscaling on test set
- [ ] Calculate cost for different strategies
- [ ] Compare cost: Fixed vs. Autoscaling vs. Minimal
- [ ] Measure SLA violations
- [ ] Optimize thresholds (grid search)
- [ ] Visualize scaling timeline
- [ ] Create cost-performance frontier plot
- [ ] Document optimal configuration

#### ‚úÖ Phase 5: Deployment
- [ ] Create FastAPI application structure
- [ ] Implement /forecast endpoint
- [ ] Implement /recommend-scaling endpoint
- [ ] Implement /metrics endpoint
- [ ] Implement /cost-report endpoint
- [ ] Test all API endpoints
- [ ] Write API documentation (OpenAPI/Swagger)
- [ ] Create Streamlit dashboard layout
- [ ] Add real-time metrics display
- [ ] Implement forecast visualization
- [ ] Add scaling recommendation widget
- [ ] Create cost analysis tab
- [ ] Add anomaly detection tab
- [ ] Test dashboard functionality
- [ ] Connect dashboard to API

#### ‚úÖ Phase 6: Documentation
- [ ] Write executive summary
- [ ] Document data analysis section
- [ ] Document methodology
- [ ] Create results section with tables/charts
- [ ] Write deployment section
- [ ] Add conclusion & future work
- [ ] Complete references
- [ ] Create appendix
- [ ] Proofread entire report (max 30 pages)
- [ ] Write comprehensive README.md
- [ ] Create presentation slides (15-20 slides)
- [ ] Record demo video (3-5 minutes)
- [ ] Edit and finalize video
- [ ] Prepare Q&A talking points

#### ‚úÖ Phase 7: MLOps & CI/CD
- [ ] Set up pre-commit hooks (black, ruff)
- [ ] Configure DVC for data versioning
- [ ] Write unit tests (80% coverage minimum)
- [ ] Create GitHub Actions CI workflow
- [ ] Build Docker images for API and Dashboard
- [ ] Test docker-compose deployment locally
- [ ] Set up MLflow Model Registry workflow
- [ ] Document deployment process

#### ‚úÖ Final Submission Checklist
- [ ] GitHub repo is public and accessible
- [ ] README.md is complete with installation guide
- [ ] All code runs without errors
- [ ] All tests pass (80%+ coverage)
- [ ] Docker containers build and run successfully
- [ ] Models are saved and documented in MLflow
- [ ] API is functional (tested with curl/Postman)
- [ ] Dashboard is functional
- [ ] Report PDF (max 30 pages)
- [ ] Presentation slides (PPTX)
- [ ] Demo video (3-5 min, MP4)
- [ ] Link GitHub repo in report
- [ ] No commits after submission deadline
- [ ] All team members reviewed final submission

---

## 7. K·∫æT LU·∫¨N & L∆ØU √ù

### 7.1 Key Success Factors

1. **Data Quality**: 100% parse success rate, proper handling of missing period
2. **Feature Engineering**: Comprehensive set of 40+ features tailored for time series
3. **Model Selection**: Balanced approach with statistical + ML models
4. **Practical Scaling**: Realistic policy with cooldown and hysteresis
5. **Clear Documentation**: Well-structured report and working demo

### 7.2 Potential Challenges & Solutions

| Challenge | Solution |
|-----------|----------|
| Missing data (Aug 1-3) | Forward fill or exclude from training |
| Model overfitting | Cross-validation with time-based split |
| Lag feature NaN | Drop initial rows or forward fill |
| API performance | Cache predictions, async processing |
| Dashboard responsiveness | Sample data for large plots |
| Cost estimation accuracy | Validate with real cloud pricing |

### 7.3 Bonus Points Opportunities

- ‚ú® **Anomaly Detection**: Implement 3-sigma spike detection + host entropy
- ‚ú® **Smart Cooldown**: Adaptive cooldown based on load volatility
- ‚ú® **Cost Optimization**: Multi-objective optimization (cost vs. performance)
- ‚ú® **Real-time Simulation**: Interactive slider in dashboard to test policies
- ‚ú® **DDoS Detection**: Low host entropy alert system
- ‚ú® **Uncertainty Quantification**: Prediction intervals using:
  - Quantile regression with LightGBM (`objective='quantile'`)
  - Conformal prediction for distribution-free intervals
  - Prophet's built-in uncertainty
- ‚ú® **SHAP Explanations**: Model interpretability with SHAP values
- ‚ú® **MLOps Pipeline**: Full MLflow + DVC + Docker + CI/CD setup
- ‚ú® **Proactive Scaling**: Predict spikes 15-30 min ahead, scale before load hits

### 7.4 Next Steps

1. **Start**: Begin with Phase 1 - Data Ingestion (Day 1)
2. **Track Progress**: Update this checklist daily
3. **Daily Stand-up**: Review completed tasks and blockers
4. **Git Commits**: Commit code at end of each phase
5. **Peer Review**: Cross-check notebooks and code quality
6. **Documentation**: Write README and comments as you code (not at the end!)

---

**Last Updated**: January 25, 2026
**Version**: 2.0
**Status**: Ready to Execute

**Changes in v2.0**:
- Added MLOps pipeline (MLflow, DVC)
- Updated dependencies to latest versions
- Added Docker containerization
- Added TimeSeriesSplit with gap for validation
- Added Optuna for hyperparameter tuning
- Added prediction intervals to forecast endpoint
- Added CI/CD with GitHub Actions
- Added SHAP for model interpretability
- Extended timeline to 19 days for testing/CI

---

## üìû CONTACT & RESOURCES

### Useful Resources
- NASA Log Dataset: http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html
- Prophet Documentation: https://facebook.github.io/prophet/
- LightGBM Documentation: https://lightgbm.readthedocs.io/
- FastAPI Documentation: https://fastapi.tiangolo.com/
- Streamlit Documentation: https://docs.streamlit.io/

### Competition Info
- Website: https://dataflow.hamictoantin.com/vi
- Fanpage: https://www.facebook.com/toantinhamic
- Email: hamic@hus.edu.vn

---

**Good luck! üçÄ**
