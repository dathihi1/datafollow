"""Feature engineering service for ML model predictions."""

from datetime import datetime, timedelta
from pathlib import Path
import json

import numpy as np
import pandas as pd

from src.features import (
    TimeFeatureExtractor,
    LagFeatureExtractor,
    RollingFeatureExtractor,
    AdvancedFeatureExtractor,
)


class FeatureService:
    """Service for computing features required by ML models.

    Transforms raw load data into the 94 features expected by the LightGBM model.
    Handles missing metadata columns by using sensible defaults.
    """

    def __init__(self, models_dir: Path | None = None):
        """Initialize feature service.

        Args:
            models_dir: Path to models directory for loading feature names
        """
        self.models_dir = models_dir
        self._feature_names: list[str] = []
        self._feature_scaler = None

        # Load expected feature names if available
        if models_dir:
            self._load_feature_config()

        # Initialize extractors
        self.time_extractor = TimeFeatureExtractor(cyclical=True)
        self.lag_extractor = LagFeatureExtractor(window="5min")
        self.rolling_extractor = RollingFeatureExtractor(time_window="5min")
        self.advanced_extractor = AdvancedFeatureExtractor()

    def _load_feature_config(self) -> None:
        """Load feature names and scaler from models directory."""
        if not self.models_dir:
            return

        # Load feature names
        feature_file = self.models_dir / "feature_names.json"
        if feature_file.exists():
            with open(feature_file, 'r') as f:
                self._feature_names = json.load(f)

        # Load scaler
        scaler_file = self.models_dir / "feature_scaler.pkl"
        if scaler_file.exists():
            try:
                import joblib
                self._feature_scaler = joblib.load(scaler_file)
            except Exception:
                pass

    def create_features(
        self,
        loads: np.ndarray,
        interval_minutes: int = 5,
        start_time: datetime | None = None,
    ) -> pd.DataFrame:
        """Create full feature set from raw load data.

        Generates all 94 features expected by LightGBM model:
        - 9 metadata features (filled with defaults)
        - 23 time features
        - 24 lag features
        - 18 rolling features
        - 20+ advanced features

        Args:
            loads: Raw load/request_count data array
            interval_minutes: Time interval between data points
            start_time: Starting timestamp (default: now minus data span)

        Returns:
            DataFrame with all features computed
        """
        n = len(loads)

        # Generate timestamps
        if start_time is None:
            start_time = datetime.now() - timedelta(minutes=interval_minutes * n)

        timestamps = [
            start_time + timedelta(minutes=interval_minutes * i)
            for i in range(n)
        ]

        # Create base DataFrame
        df = pd.DataFrame({
            "timestamp": timestamps,
            "request_count": loads,
        })

        # Add metadata columns with defaults (these aren't available from raw loads)
        df = self._add_default_metadata(df)

        # Extract all feature groups
        df = self.time_extractor.transform(df, timestamp_col="timestamp")
        df = self.lag_extractor.transform_with_diff(df, target_cols=["request_count"])
        df = self._add_bytes_lag_features(df)  # bytes_total lags
        df = self.rolling_extractor.transform(df, target_cols=["request_count"])
        df = self.rolling_extractor.transform_ewm(df, target_cols=["request_count"])
        df = self.advanced_extractor.transform(df)
        df = self._add_anomaly_features(df)

        # Fill NaN from lag/rolling computations
        # Handle categorical columns separately
        for col in df.columns:
            if df[col].dtype == 'category':
                # For categorical, fill with mode or first category
                if df[col].isna().any():
                    fill_val = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else df[col].cat.categories[0]
                    df[col] = df[col].fillna(fill_val)
            elif df[col].dtype in [np.float64, np.int64, np.float32, np.int32]:
                df[col] = df[col].fillna(0)

        return df

    def _add_default_metadata(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add default values for metadata columns not available from raw loads.

        These features require the original log parsing to compute accurately.
        We use sensible defaults based on typical patterns.
        """
        n = len(df)
        request_count = df["request_count"].values

        # Estimate unique hosts (typically 60-80% of request count, min 1)
        df["unique_hosts"] = np.maximum(1, (request_count * 0.7).astype(int))

        # Estimate error count (typically 1-5% of requests)
        df["error_count"] = np.maximum(0, (request_count * 0.02).astype(int))
        df["error_rate"] = df["error_count"] / np.maximum(1, request_count)
        df["success_rate"] = 1 - df["error_rate"]

        # Estimate bytes (average ~5KB per request for web traffic)
        avg_bytes_per_request = 5000
        df["bytes_total"] = request_count * avg_bytes_per_request
        df["bytes_avg"] = avg_bytes_per_request
        df["bytes_max"] = avg_bytes_per_request * 10  # Max ~10x average

        # Derived metrics
        df["requests_per_host"] = request_count / np.maximum(1, df["unique_hosts"])
        df["bytes_per_request"] = df["bytes_total"] / np.maximum(1, request_count)

        return df

    def _add_bytes_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add lag features for bytes_total column."""
        if "bytes_total" not in df.columns:
            return df

        lags = [1, 3, 6, 12, 60, 288]
        for lag in lags:
            df[f"bytes_total_lag_{lag}"] = df["bytes_total"].shift(lag)

        return df

    def _add_anomaly_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add anomaly detection features.

        Uses simple statistical methods since ML-based anomaly detection
        requires additional dependencies.
        """
        if "request_count" not in df.columns:
            return df

        col = df["request_count"]

        # Rolling z-score based anomaly detection
        rolling_mean = col.shift(1).rolling(window=60, min_periods=1).mean()
        rolling_std = col.shift(1).rolling(window=60, min_periods=1).std().replace(0, 1)

        z_score = (col - rolling_mean) / rolling_std

        # Anomaly flags
        df["is_anomaly_ml"] = (np.abs(z_score) > 3).astype(int)
        df["anomaly_score_ml"] = np.abs(z_score) / 3  # Normalized score

        # Agreement between different methods (spike + z-score)
        if "is_spike" in df.columns:
            df["anomaly_agreement"] = (
                (df["is_spike"] == 1) & (df["is_anomaly_ml"] == 1)
            ).astype(int)
        else:
            df["anomaly_agreement"] = 0

        return df

    def prepare_for_prediction(
        self,
        df: pd.DataFrame,
        scale: bool = True,
    ) -> np.ndarray:
        """Prepare features for model prediction.

        Selects required features in correct order and optionally scales.

        Args:
            df: DataFrame with all computed features
            scale: Whether to apply feature scaling

        Returns:
            Feature array ready for model.predict()
        """
        if not self._feature_names:
            # Use all numeric columns except timestamp
            feature_cols = [
                c for c in df.columns
                if c not in ["timestamp", "time_of_day", "part_of_day",
                            "event_impact", "event_name"]
                and df[c].dtype in [np.float64, np.int64, np.float32, np.int32]
            ]
        else:
            feature_cols = self._feature_names

        # Select features, filling missing with 0
        X = pd.DataFrame()
        for col in feature_cols:
            if col in df.columns:
                X[col] = df[col]
            else:
                X[col] = 0

        # Convert to array
        X_arr = X.values.astype(np.float64)

        # Replace inf/nan
        X_arr = np.nan_to_num(X_arr, nan=0.0, posinf=0.0, neginf=0.0)

        # Scale if scaler available
        if scale and self._feature_scaler is not None:
            try:
                X_arr = self._feature_scaler.transform(X_arr)
            except Exception:
                pass

        return X_arr

    def create_iterative_forecast(
        self,
        historical_loads: np.ndarray,
        model,
        horizon: int,
        interval_minutes: int = 5,
    ) -> np.ndarray:
        """Generate forecast using optimized iterative prediction.

        Args:
            historical_loads: Historical load data
            model: Trained model with predict() method
            horizon: Number of future periods to forecast
            interval_minutes: Time interval between points

        Returns:
            Array of predictions
        """
        predictions = []

        # Pre-compute historical features once
        hist_df = self.create_features(
            loads=historical_loads,
            interval_minutes=interval_minutes,
        )

        # Calculate trend from recent history
        recent_window = min(288, len(historical_loads))
        recent_trend = self._calculate_trend(historical_loads[-recent_window:])

        # Get last timestamp
        last_time = hist_df["timestamp"].iloc[-1]

        for step in range(horizon):
            future_time = last_time + timedelta(minutes=interval_minutes * (step + 1))

            # Create features for this single forecast point
            row = self._create_forecast_row(
                timestamp=future_time,
                historical_df=hist_df,
                historical_values=historical_loads,  # Use full historical data
                step=step,
                interval_minutes=interval_minutes,
            )

            # Prepare and predict
            features = self.prepare_for_prediction(row, scale=True)
            pred = model.predict(features)[0]

            # Apply trend and noise
            pred = self._apply_trend_and_noise(pred, step, recent_trend, historical_loads)
            pred = max(0, pred)

            predictions.append(pred)

        return np.array(predictions)

    def _create_forecast_row(
        self,
        timestamp: datetime,
        historical_df: pd.DataFrame,
        historical_values: np.ndarray,
        step: int,
        interval_minutes: int,
    ) -> pd.DataFrame:
        """Create feature row for a single forecast point.

        Uses historical data patterns for lag/rolling features.
        For better accuracy, matches seasonal patterns from same time of day/week.
        """
        row = pd.DataFrame([{"timestamp": timestamp}])

        # Time features
        row = self.time_extractor.transform(row, timestamp_col="timestamp")

        # Get seasonality pattern - same hour/day from history
        hour_of_day = timestamp.hour
        day_of_week = timestamp.weekday()
        
        # Find similar periods in history (same hour, preferably same day of week)
        hist_timestamps = historical_df["timestamp"]
        similar_mask = (hist_timestamps.dt.hour == hour_of_day)
        
        if similar_mask.sum() > 0:
            similar_vals = historical_values[similar_mask]
            seasonal_estimate = np.median(similar_vals)  # Robust to outliers
            seasonal_std = np.std(similar_vals)
        else:
            seasonal_estimate = np.mean(historical_values[-288:]) if len(historical_values) >= 288 else np.mean(historical_values)
            seasonal_std = np.std(historical_values[-288:]) if len(historical_values) >= 288 else np.std(historical_values)

        n_hist = len(historical_values)
        
        # Recent trend component
        recent_vals = historical_values[-min(12, n_hist):]
        recent_mean = np.mean(recent_vals)
        
        # Combine seasonal pattern with recent trend (weighted average)
        # 70% seasonal, 30% recent trend
        current_estimate = 0.7 * seasonal_estimate + 0.3 * recent_mean

        # Lag features - use seasonal pattern lookback
        lags = [1, 3, 6, 12, 60, 288]
        for lag in lags:
            # Look back in historical data using seasonal pattern
            lookback_time = timestamp - timedelta(minutes=interval_minutes * lag)
            lookback_hour = lookback_time.hour
            
            # Find similar hours in history
            hour_mask = (hist_timestamps.dt.hour == lookback_hour)
            if hour_mask.sum() > 0:
                lag_val = np.median(historical_values[hour_mask])
            else:
                # Fallback to last known value with lag
                idx = n_hist - lag
                lag_val = historical_values[idx] if idx >= 0 else historical_values[0]
            
            row[f"request_count_lag_{lag}"] = lag_val
            
            # Diff and pct_change based on estimates
            row[f"request_count_diff_{lag}"] = current_estimate - lag_val
            if lag_val > 0:
                row[f"request_count_pct_change_{lag}"] = (current_estimate - lag_val) / lag_val
            else:
                row[f"request_count_pct_change_{lag}"] = 0

        # Rolling features - use seasonal pattern from similar hours
        # This ensures variation across forecast points
        windows = [3, 12, 60]
        for window in windows:
            # Get values from similar hours in history for rolling window
            window_vals = []
            for w in range(window):
                lookback_hour = (hour_of_day - w) % 24
                hour_mask = (hist_timestamps.dt.hour == lookback_hour)
                if hour_mask.sum() > 0:
                    window_vals.append(np.median(historical_values[hour_mask]))
                else:
                    window_vals.append(current_estimate)

            window_vals = np.array(window_vals)
            row[f"request_count_rolling_mean_{window}"] = np.mean(window_vals)
            row[f"request_count_rolling_std_{window}"] = np.std(window_vals) if len(window_vals) > 1 else seasonal_std
            if window <= 12:
                row[f"request_count_rolling_min_{window}"] = np.min(window_vals)
                row[f"request_count_rolling_max_{window}"] = np.max(window_vals)
                row[f"request_count_rolling_range_{window}"] = np.max(window_vals) - np.min(window_vals)

        # EWM features - use seasonal pattern with exponential weighting
        for span in [5, 15, 30]:
            ewm_vals = []
            for w in range(min(span * 2, 48)):  # Cap at 48 periods
                lookback_hour = (hour_of_day - w) % 24
                hour_mask = (hist_timestamps.dt.hour == lookback_hour)
                if hour_mask.sum() > 0:
                    ewm_vals.append(np.median(historical_values[hour_mask]))
                else:
                    ewm_vals.append(current_estimate)

            ewm_vals = np.array(ewm_vals)
            if len(ewm_vals) > 0:
                weights = np.exp(-np.arange(len(ewm_vals)) / span)
                weights /= weights.sum()
                row[f"request_count_ewm_mean_{span}"] = np.sum(ewm_vals * weights)
                row[f"request_count_ewm_std_{span}"] = np.std(ewm_vals)
            else:
                row[f"request_count_ewm_mean_{span}"] = current_estimate
                row[f"request_count_ewm_std_{span}"] = seasonal_std

        # Metadata defaults - scale with estimate
        row["unique_hosts"] = int(current_estimate * 0.7) if current_estimate > 0 else 100
        row["error_count"] = int(current_estimate * 0.02)
        row["error_rate"] = 0.02
        row["success_rate"] = 0.98
        row["bytes_total"] = current_estimate * 5000
        row["bytes_avg"] = 5000
        row["bytes_max"] = 50000
        row["requests_per_host"] = 1.4
        row["bytes_per_request"] = 5000

        # Bytes lag features
        for lag in lags:
            row[f"bytes_total_lag_{lag}"] = row["bytes_total"].iloc[0]

        # Advanced features - use recent patterns
        row["spike_score"] = 0
        row["is_spike"] = 0
        row["is_dip"] = 0
        row["spike_magnitude"] = 0
        row["request_count_direction"] = 0
        row["request_count_streak"] = 0
        row["request_count_trend"] = 0
        row["request_count_vs_yesterday"] = 0
        row["request_count_vs_yesterday_pct"] = 0
        row["request_count_cv"] = seasonal_std / (seasonal_estimate + 1e-6) if seasonal_estimate > 0 else 0.2
        row["request_count_bb_upper"] = current_estimate * 1.5
        row["request_count_bb_lower"] = current_estimate * 0.5
        row["request_count_bb_width"] = current_estimate
        row["request_count_velocity"] = 0
        row["request_count_acceleration"] = 0
        row["request_count_momentum"] = 0

        # Event features
        row["event_type"] = 0
        row["is_special_event"] = 0

        # Anomaly features
        row["is_anomaly_ml"] = 0
        row["anomaly_score_ml"] = 0
        row["anomaly_agreement"] = 0

        return row

    def _calculate_trend(self, recent_data: np.ndarray) -> float:
        """Calculate trend coefficient from recent data.
        
        Uses linear regression to find slope.
        Positive = increasing, Negative = decreasing
        """
        if len(recent_data) < 10:
            return 0.0
        
        # Simple linear regression
        x = np.arange(len(recent_data))
        y = recent_data
        
        # Calculate slope
        mean_x = np.mean(x)
        mean_y = np.mean(y)
        numerator = np.sum((x - mean_x) * (y - mean_y))
        denominator = np.sum((x - mean_x) ** 2)
        
        if denominator == 0:
            return 0.0
        
        slope = numerator / denominator
        return slope

    def _apply_trend_and_noise(
        self,
        prediction: float,
        step: int,
        trend: float,
        historical_data: np.ndarray,
    ) -> float:
        """Apply trend, daily variation, and realistic noise to prediction.
        
        Args:
            prediction: Raw model prediction
            step: Current forecast step (0-indexed)
            trend: Trend coefficient from recent history
            historical_data: Historical values for variance estimation
            
        Returns:
            Adjusted prediction with trend and noise
        """
        # Calculate which day we're forecasting
        day_number = step // 288  # Which day (0, 1, 2, ...)
        
        # Apply trend (attenuate over distance)
        trend_factor = trend * step * (1.0 - step / 5000)  # Accumulative trend
        prediction_with_trend = prediction + trend_factor
        
        # Daily variation factor - each day is slightly different
        # This breaks the perfect cycle
        np.random.seed(42 + day_number)  # Consistent per day but different between days
        daily_scale = 1.0 + np.random.uniform(-0.15, 0.15)  # Â±15% daily variation
        prediction_with_daily = prediction_with_trend * daily_scale
        
        # Reset random seed for noise
        np.random.seed(None)
        
        # Add realistic noise based on historical variance
        hist_std = np.std(historical_data[-288:]) if len(historical_data) >= 288 else np.std(historical_data)
        
        # Larger noise for more realistic variation
        noise_level = hist_std * 0.15  # 15% of historical std
        noise = np.random.normal(0, noise_level)
        
        # Add occasional spikes/dips (5% chance)
        if np.random.random() < 0.05:
            spike_magnitude = np.random.choice([-1, 1]) * np.random.uniform(0.2, 0.5) * prediction
            noise += spike_magnitude
        
        final_prediction = prediction_with_daily + noise
        
        return final_prediction

    def create_forecast_features(
        self,
        historical_loads: np.ndarray,
        horizon: int,
        interval_minutes: int = 5,
    ) -> tuple[pd.DataFrame, np.ndarray]:
        """Legacy method - kept for compatibility.
        
        NOTE: Use create_iterative_forecast() for better results.
        This method is kept for models that don't support iterative forecasting.
        
        Args:
            historical_loads: Historical load data
            horizon: Number of future periods to forecast
            interval_minutes: Time interval between points

        Returns:
            Tuple of (historical_features_df, forecast_feature_array)
        """
        # Create features for historical data
        hist_df = self.create_features(
            historical_loads,
            interval_minutes=interval_minutes,
        )

        # For forecasting, we need to iteratively generate features
        # using seasonality patterns from historical data
        forecast_features = []

        # Get last timestamp
        last_time = hist_df["timestamp"].iloc[-1]
        
        # Use recent historical values for pattern matching
        hist_vals = hist_df["request_count"].values
        n_hist = len(hist_vals)

        for i in range(horizon):
            future_time = last_time + timedelta(minutes=interval_minutes * (i + 1))

            # Create single-row DataFrame for this forecast point
            row = self._create_forecast_row(
                timestamp=future_time,
                historical_df=hist_df,
                historical_values=hist_vals,
                step=i,
                interval_minutes=interval_minutes,
            )
            forecast_features.append(row)

        forecast_df = pd.concat(forecast_features, ignore_index=True)

        # Prepare feature arrays
        forecast_X = self.prepare_for_prediction(forecast_df, scale=True)

        return hist_df, forecast_X
