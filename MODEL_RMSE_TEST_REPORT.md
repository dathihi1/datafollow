# ğŸ“Š MODEL RMSE TEST RESULTS

**NgÃ y test**: 31 ThÃ¡ng 1, 2026  
**Dataset**: NASA Web Server Logs (5min aggregation)  
**Test size**: 9 ngÃ y (Aug 23-31, 1995)

---

## ğŸ¯ TÃ“M Táº®T Káº¾T QUáº¢

### **Best Model: Prophet** 
- âœ… Test RMSE: **139.19**
- âœ… Test MAE: 102.52
- âœ… Generalization ratio: 1.34x (good)

### **Baseline: SARIMA**
- âœ… Test RMSE: **150.37**
- âœ… Test MAE: 108.56
- âœ… Generalization ratio: 1.01x (excellent)

### **Problem: LightGBM**
- âŒ Test RMSE: **262.65** (worse than baselines!)
- âŒ Validation RMSE: 0.53 (too perfect)
- âŒ Overfitting ratio: **497x** (severe)

---

## ğŸ“ˆ CHI TIáº¾T PERFORMANCE

### Test Set Performance (9 ngÃ y test data)

| Model | RMSE | MAE | RÂ² | Status |
|-------|------|-----|-----|--------|
| **Prophet** | **139.19** âœ… | 102.52 | -0.29 | **BEST** |
| **SARIMA** | **150.37** âœ… | 108.56 | -0.50 | Good |
| **LightGBM** | **262.65** âŒ | 235.24 | -3.59 | **OVERFITTED** |

### Validation Set Performance (train subset)

| Model | RMSE | MAE | RÂ² | Status |
|-------|------|-----|-----|--------|
| Prophet | 103.57 | 88.59 | -0.12 | Good |
| SARIMA | 148.37 | 114.29 | -1.30 | Good |
| LightGBM | **0.53** âš ï¸ | 0.37 | **1.00** | Too perfect! |

---

## ğŸ” OVERFITTING ANALYSIS

### Test/Validation RMSE Ratio

```
Prophet:  1.34x  âœ… Good generalization
SARIMA:   1.01x  âœ… Excellent generalization  
LightGBM: 497x   âŒ SEVERE OVERFITTING!
```

**Thresholds:**
- âœ… **< 1.5x**: Good generalization
- âš ï¸ **1.5-2.0x**: Moderate overfitting
- âŒ **> 2.0x**: Severe overfitting

---

## ğŸ“Š VISUALIZATION

Xem biá»ƒu Ä‘á»“ so sÃ¡nh táº¡i: [reports/figures/model_rmse_comparison.png](reports/figures/model_rmse_comparison.png)

**3 charts:**
1. **Test RMSE Comparison** - So sÃ¡nh RMSE test set
2. **Val vs Test RMSE** - So sÃ¡nh validation vÃ  test
3. **Overfitting Analysis** - PhÃ¢n tÃ­ch má»©c Ä‘á»™ overfit

---

## ğŸ¯ RANKING & COMPARISON

### 1. Prophet (WINNER ğŸ†)
```
âœ… Pros:
   - Best test RMSE (139.19)
   - Good generalization (1.34x ratio)
   - Stable performance
   - RÂ² close to 0 (reasonable for time series)

âš ï¸ Cons:
   - Slower training than ML models
   - Less flexible than gradient boosting
```

### 2. SARIMA (Solid Baseline â­)
```
âœ… Pros:
   - Excellent generalization (1.01x ratio)
   - Statistical foundation
   - Interpretable
   - Test RMSE only 8% worse than Prophet

âš ï¸ Cons:
   - Slower inference
   - Limited feature engineering
   - Fixed seasonal patterns
```

### 3. LightGBM (NEEDS FIX âŒ)
```
âŒ Critical Issues:
   - Severe overfitting (497x ratio!)
   - Test RMSE worse than baselines
   - Val RMSE = 0.53 (memorized training data)
   - RÂ² = -3.59 on test (terrible)

ğŸ”§ Root Causes (identified):
   1. Data leakage: feature 'request_count_pct_of_max'
   2. Weak regularization: reg_lambda = 0.0004
   3. Model too complex: num_leaves = 201

ğŸ“‹ Action Required:
   See FIX_LIGHTGBM_PROMPT.md for detailed fix plan
```

---

## ğŸ“‰ SO SÃNH Vá»šI COLAB NOTEBOOK

### Colab Results (tá»« Google Colab)

| Dataset | Model | RMSE | Status |
|---------|-------|------|--------|
| 5min | Prophet | 762.93 | âŒ Worse |
| 5min | XGBoost | **53.02** | âœ… **Excellent** |
| 1min | XGBoost | **15.04** | âœ… Excellent |
| 15min | XGBoost | 127.51 | âœ… Good |

### So sÃ¡nh cá»¥ thá»ƒ (5min aggregation)

| Metric | Colab Prophet | Project Prophet | Winner |
|--------|---------------|-----------------|--------|
| Test RMSE | 762.93 | **139.19** | ğŸ† **Project (5.5x better!)** |
| Generalization | âš ï¸ Moderate | âœ… Good | ğŸ† Project |

| Metric | Colab XGBoost | Project LightGBM | Winner |
|--------|---------------|------------------|--------|
| Test RMSE | **53.02** | 262.65 | ğŸ† **Colab (5x better!)** |
| Generalization | âœ… Good | âŒ Severe overfit | ğŸ† Colab |

**Key Insight:**
- âœ… Dá»± Ã¡n Prophet Tá»T HÆ N NHIá»€U so vá»›i Colab (139 vs 762)
- âŒ Dá»± Ã¡n LightGBM Tá»† HÆ N NHIá»€U so vá»›i Colab XGBoost (262 vs 53)
- ğŸ¯ **Action**: Fix LightGBM hoáº·c thÃªm XGBoost vÃ o project

---

## ğŸ”§ NEXT STEPS

### Priority 1: Fix LightGBM (2-3 giá»)
1. âœ… ÄÃ£ phÃ¢n tÃ­ch root cause
2. âœ… ÄÃ£ táº¡o FIX_LIGHTGBM_PROMPT.md
3. â³ Cáº§n implement fixes:
   - Remove data leakage feature
   - Fix Optuna search space
   - Add stronger regularization
   - Re-train and validate

**Expected after fix:**
- Target Test RMSE: < 140 (better than Prophet)
- Target Val/Test ratio: < 1.5x
- Target RÂ²: > 0.5

### Priority 2: Add XGBoost (1-2 giá»)
1. Port code from Colab notebook
2. Create `src/models/xgboost_model.py`
3. Benchmark against other models
4. Expected: Test RMSE ~ 50-60 (nhÆ° Colab)

### Priority 3: Ensemble Model (Optional)
1. Combine Prophet + XGBoost
2. Weighted average or stacking
3. Potential: Test RMSE < 100

---

## ğŸ“ TEST COMMANDS

### Run full benchmark:
```bash
cd c:\Users\Admin\OneDrive\Documents\python\datafollow

# Prophet + SARIMA
jupyter notebook notebooks/06_baseline_models.ipynb

# LightGBM
jupyter notebook notebooks/07_ml_models.ipynb

# All models comparison
jupyter notebook notebooks/11_final_benchmark.ipynb
```

### Quick RMSE check:
```bash
# Via Python
python -c "
import json
with open('models/all_model_results.json', 'r') as f:
    r = json.load(f)
    print(f'Prophet: {r['prophet_test']['rmse']:.2f}')
    print(f'SARIMA:  {r['sarima_test']['rmse']:.2f}')
    print(f'LightGBM: {r['lgbm_test']['rmse']:.2f}')
"

# Via API
curl http://localhost:8000/metrics
```

---

## ğŸ“ LEARNINGS & INSIGHTS

### What Worked Well âœ…
1. **Prophet**: Excellent for this use case
   - Handles trend and seasonality automatically
   - Robust to outliers (STS-70 launch, hurricane)
   - Good generalization
   
2. **SARIMA**: Solid statistical baseline
   - Perfect generalization (1.01x)
   - Good for understanding data patterns

3. **Feature Engineering**: 87 features created
   - Time features (cyclical encoding)
   - Lag features (1-288 periods)
   - Rolling statistics
   - Special events dictionary
   - IsolationForest anomaly detection

### What Needs Improvement âŒ
1. **LightGBM Tuning**: 
   - Optuna search space too permissive
   - Needs stronger regularization constraints
   - Data leakage in features

2. **Model Selection**:
   - Should benchmark more models (XGBoost, LSTM)
   - Should try ensemble methods

3. **Hyperparameter Tuning**:
   - Need better validation strategy
   - Should use TimeSeriesSplit CV

---

## ğŸ“š REFERENCES

### Files
- Model results: `models/all_model_results.json`
- Benchmark CSV: `reports/benchmark_results.csv`
- Visualization: `reports/figures/model_rmse_comparison.png`
- Fix guide: `FIX_LIGHTGBM_PROMPT.md`
- Comparison report: `COMPARISON_REPORT.md`

### Notebooks
- Baseline models: `notebooks/06_baseline_models.ipynb`
- ML models: `notebooks/07_ml_models.ipynb`
- Final benchmark: `notebooks/11_final_benchmark.ipynb`

### Code
- Prophet: `src/models/prophet_model.py`
- SARIMA: `src/models/sarima.py`
- LightGBM: `src/models/lgbm_model.py`

---

**TÃ¡c giáº£**: GitHub Copilot  
**NgÃ y**: 31 ThÃ¡ng 1, 2026  
**Version**: 1.0

**Status**: 
- âœ… Prophet: Production-ready
- âœ… SARIMA: Production-ready
- âŒ LightGBM: Needs fixing
