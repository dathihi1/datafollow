{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Final Benchmark Report\n",
    "\n",
    "Comprehensive benchmark of all models across all time granularities.\n",
    "\n",
    "**Models:**\n",
    "- SARIMA (Statistical baseline)\n",
    "- Prophet (Facebook's time series model)\n",
    "- LightGBM (Gradient boosting)\n",
    "\n",
    "**Granularities:**\n",
    "- 1-minute\n",
    "- 5-minute\n",
    "- 15-minute\n",
    "\n",
    "**Metrics:**\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- MAPE (Mean Absolute Percentage Error)\n",
    "- R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Project imports\n",
    "from src.utils.metrics import evaluate_forecast\n",
    "from src.models.prophet_model import ProphetModel\n",
    "from src.models.lgbm_model import LGBMModel\n",
    "from src.models.sarima import SARIMAModel\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = project_root / 'DATA' / 'processed'\n",
    "MODELS_DIR = project_root / 'models'\n",
    "REPORTS_DIR = project_root / 'reports'\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Benchmark started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all granularities\n",
    "datasets = {}\n",
    "\n",
    "for granularity in ['1m', '5m', '15m']:\n",
    "    train_path = DATA_DIR / f'train_{granularity}.parquet'\n",
    "    test_path = DATA_DIR / f'test_{granularity}.parquet'\n",
    "    \n",
    "    if train_path.exists() and test_path.exists():\n",
    "        datasets[granularity] = {\n",
    "            'train': pd.read_parquet(train_path),\n",
    "            'test': pd.read_parquet(test_path)\n",
    "        }\n",
    "        print(f\"Loaded {granularity}: Train {datasets[granularity]['train'].shape}, Test {datasets[granularity]['test'].shape}\")\n",
    "    else:\n",
    "        print(f\"Missing data for {granularity}\")\n",
    "\n",
    "# Also load feature-engineered 5m data\n",
    "if (DATA_DIR / 'train_features_5m.parquet').exists():\n",
    "    datasets['5m_features'] = {\n",
    "        'train': pd.read_parquet(DATA_DIR / 'train_features_5m.parquet'),\n",
    "        'test': pd.read_parquet(DATA_DIR / 'test_features_5m.parquet')\n",
    "    }\n",
    "    print(f\"Loaded 5m_features: Train {datasets['5m_features']['train'].shape}, Test {datasets['5m_features']['test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_prophet(train_df, test_df, granularity):\n",
    "    \"\"\"Train Prophet and evaluate on test set.\"\"\"\n",
    "    try:\n",
    "        # Prepare data for Prophet\n",
    "        train_prophet = train_df[['timestamp', 'request_count']].rename(\n",
    "            columns={'timestamp': 'ds', 'request_count': 'y'}\n",
    "        )\n",
    "        \n",
    "        # Train Prophet\n",
    "        model = ProphetModel()\n",
    "        model.fit(train_prophet)\n",
    "        \n",
    "        # Predict\n",
    "        future = test_df[['timestamp']].rename(columns={'timestamp': 'ds'})\n",
    "        predictions = model.predict(future)['yhat'].values\n",
    "        \n",
    "        # Evaluate\n",
    "        y_true = test_df['request_count'].values\n",
    "        metrics = evaluate_forecast(y_true, predictions, name='Prophet')\n",
    "        metrics['granularity'] = granularity\n",
    "        \n",
    "        return metrics, predictions\n",
    "    except Exception as e:\n",
    "        print(f\"Prophet error ({granularity}): {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_and_evaluate_sarima(train_df, test_df, granularity):\n",
    "    \"\"\"Train SARIMA and evaluate on test set.\"\"\"\n",
    "    try:\n",
    "        # Use request_count series\n",
    "        train_series = train_df['request_count'].values\n",
    "        \n",
    "        # Train SARIMA with appropriate seasonal period\n",
    "        seasonal_periods = {'1m': 60, '5m': 12, '15m': 4}  # hourly seasonality\n",
    "        period = seasonal_periods.get(granularity, 12)\n",
    "        \n",
    "        model = SARIMAModel(order=(1, 1, 1), seasonal_order=(1, 1, 1, period))\n",
    "        model.fit(train_series)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(steps=len(test_df))\n",
    "        \n",
    "        # Evaluate\n",
    "        y_true = test_df['request_count'].values\n",
    "        metrics = evaluate_forecast(y_true, predictions, name='SARIMA')\n",
    "        metrics['granularity'] = granularity\n",
    "        \n",
    "        return metrics, predictions\n",
    "    except Exception as e:\n",
    "        print(f\"SARIMA error ({granularity}): {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_and_evaluate_lgbm(train_df, test_df, granularity, feature_cols=None):\n",
    "    \"\"\"Train LightGBM and evaluate on test set.\"\"\"\n",
    "    try:\n",
    "        # Determine feature columns\n",
    "        if feature_cols is None:\n",
    "            # Use basic features if no feature columns provided\n",
    "            numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            feature_cols = [c for c in numeric_cols if c != 'request_count']\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train = train_df[feature_cols].fillna(0)\n",
    "        y_train = train_df['request_count']\n",
    "        X_test = test_df[feature_cols].fillna(0)\n",
    "        y_test = test_df['request_count']\n",
    "        \n",
    "        # Train LightGBM\n",
    "        model = LGBMModel()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_forecast(y_test.values, predictions, name='LightGBM')\n",
    "        metrics['granularity'] = granularity\n",
    "        \n",
    "        return metrics, predictions\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM error ({granularity}): {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = []\n",
    "predictions_store = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RUNNING COMPREHENSIVE BENCHMARKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for granularity in ['1m', '5m', '15m']:\n",
    "    if granularity not in datasets:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Benchmarking: {granularity}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_df = datasets[granularity]['train']\n",
    "    test_df = datasets[granularity]['test']\n",
    "    \n",
    "    predictions_store[granularity] = {'actual': test_df['request_count'].values}\n",
    "    \n",
    "    # Prophet\n",
    "    print(\"\\nTraining Prophet...\")\n",
    "    prophet_metrics, prophet_pred = train_and_evaluate_prophet(train_df, test_df, granularity)\n",
    "    if prophet_metrics:\n",
    "        all_results.append(prophet_metrics)\n",
    "        predictions_store[granularity]['prophet'] = prophet_pred\n",
    "        print(f\"  Prophet RMSE: {prophet_metrics['rmse']:.2f}\")\n",
    "    \n",
    "    # SARIMA (skip for 1m due to computation time)\n",
    "    if granularity != '1m':\n",
    "        print(\"\\nTraining SARIMA...\")\n",
    "        sarima_metrics, sarima_pred = train_and_evaluate_sarima(train_df, test_df, granularity)\n",
    "        if sarima_metrics:\n",
    "            all_results.append(sarima_metrics)\n",
    "            predictions_store[granularity]['sarima'] = sarima_pred\n",
    "            print(f\"  SARIMA RMSE: {sarima_metrics['rmse']:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping SARIMA for 1m (too slow)\")\n",
    "    \n",
    "    # LightGBM (only for 5m with features)\n",
    "    if granularity == '5m' and '5m_features' in datasets:\n",
    "        print(\"\\nTraining LightGBM (with features)...\")\n",
    "        train_feat = datasets['5m_features']['train']\n",
    "        test_feat = datasets['5m_features']['test']\n",
    "        \n",
    "        # Load feature names\n",
    "        try:\n",
    "            with open(MODELS_DIR / 'feature_names.json', 'r') as f:\n",
    "                feature_cols = json.load(f)\n",
    "        except:\n",
    "            feature_cols = None\n",
    "        \n",
    "        lgbm_metrics, lgbm_pred = train_and_evaluate_lgbm(train_feat, test_feat, granularity, feature_cols)\n",
    "        if lgbm_metrics:\n",
    "            all_results.append(lgbm_metrics)\n",
    "            predictions_store[granularity]['lgbm'] = lgbm_pred\n",
    "            print(f\"  LightGBM RMSE: {lgbm_metrics['rmse']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARKS COMPLETED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Reorder columns\n",
    "cols_order = ['granularity', 'model', 'rmse', 'mae', 'mape', 'r2']\n",
    "cols_available = [c for c in cols_order if c in results_df.columns]\n",
    "results_df = results_df[cols_available]\n",
    "\n",
    "# Sort by granularity and RMSE\n",
    "results_df = results_df.sort_values(['granularity', 'rmse'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL BENCHMARK RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "results_path = REPORTS_DIR / 'benchmark_results.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "\n",
    "# Also save as JSON for API consumption\n",
    "results_json = results_df.to_dict(orient='records')\n",
    "with open(REPORTS_DIR / 'benchmark_results.json', 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "print(f\"JSON saved to: {REPORTS_DIR / 'benchmark_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Color palette\n",
    "colors = {'Prophet': '#A23B72', 'SARIMA': '#F18F01', 'LightGBM': '#2ECC71'}\n",
    "\n",
    "# 1. RMSE by Model and Granularity\n",
    "ax = axes[0, 0]\n",
    "pivot_rmse = results_df.pivot(index='granularity', columns='model', values='rmse')\n",
    "pivot_rmse.plot(kind='bar', ax=ax, color=[colors.get(c, '#2E86AB') for c in pivot_rmse.columns], alpha=0.8, edgecolor='black')\n",
    "ax.set_title('RMSE by Model and Granularity', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Granularity')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend(title='Model')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. MAE by Model and Granularity\n",
    "ax = axes[0, 1]\n",
    "pivot_mae = results_df.pivot(index='granularity', columns='model', values='mae')\n",
    "pivot_mae.plot(kind='bar', ax=ax, color=[colors.get(c, '#2E86AB') for c in pivot_mae.columns], alpha=0.8, edgecolor='black')\n",
    "ax.set_title('MAE by Model and Granularity', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Granularity')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.legend(title='Model')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 3. MAPE by Model and Granularity\n",
    "ax = axes[1, 0]\n",
    "pivot_mape = results_df.pivot(index='granularity', columns='model', values='mape')\n",
    "pivot_mape.plot(kind='bar', ax=ax, color=[colors.get(c, '#2E86AB') for c in pivot_mape.columns], alpha=0.8, edgecolor='black')\n",
    "ax.set_title('MAPE (%) by Model and Granularity', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Granularity')\n",
    "ax.set_ylabel('MAPE (%)')\n",
    "ax.legend(title='Model')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 4. Best Model Summary\n",
    "ax = axes[1, 1]\n",
    "best_by_granularity = results_df.loc[results_df.groupby('granularity')['rmse'].idxmin()]\n",
    "bars = ax.bar(best_by_granularity['granularity'], best_by_granularity['rmse'], \n",
    "              color=[colors.get(m, '#2E86AB') for m in best_by_granularity['model']], \n",
    "              alpha=0.8, edgecolor='black')\n",
    "ax.set_title('Best Model by Granularity (Lowest RMSE)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Granularity')\n",
    "ax.set_ylabel('RMSE')\n",
    "\n",
    "# Add model labels\n",
    "for bar, model, rmse in zip(bars, best_by_granularity['model'], best_by_granularity['rmse']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "            f'{model}\\n({rmse:.1f})', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'figures' / 'benchmark_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to: reports/figures/benchmark_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of all metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = results_df.copy()\n",
    "heatmap_data['model_gran'] = heatmap_data['model'] + ' (' + heatmap_data['granularity'] + ')'\n",
    "heatmap_data = heatmap_data.set_index('model_gran')[['rmse', 'mae', 'mape']]\n",
    "\n",
    "# Normalize for better visualization\n",
    "heatmap_normalized = heatmap_data.apply(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-10))\n",
    "\n",
    "sns.heatmap(heatmap_normalized, annot=heatmap_data.values, fmt='.1f', \n",
    "            cmap='RdYlGn_r', ax=ax, cbar_kws={'label': 'Normalized Score (lower is better)'})\n",
    "ax.set_title('Model Performance Heatmap (values are actual metrics)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Model (Granularity)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'figures' / 'benchmark_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictions Visualization (5-minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for 5-minute data\n",
    "if '5m' in predictions_store:\n",
    "    pred_5m = predictions_store['5m']\n",
    "    test_5m = datasets['5m']['test']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(18, 10))\n",
    "    \n",
    "    # Full period\n",
    "    ax = axes[0]\n",
    "    ax.plot(test_5m['timestamp'], pred_5m['actual'], label='Actual', color='black', linewidth=1.5)\n",
    "    \n",
    "    if 'prophet' in pred_5m:\n",
    "        ax.plot(test_5m['timestamp'], pred_5m['prophet'], label='Prophet', \n",
    "                color='#A23B72', linewidth=1, linestyle='--', alpha=0.8)\n",
    "    if 'sarima' in pred_5m:\n",
    "        ax.plot(test_5m['timestamp'], pred_5m['sarima'], label='SARIMA', \n",
    "                color='#F18F01', linewidth=1, linestyle=':', alpha=0.8)\n",
    "    if 'lgbm' in pred_5m:\n",
    "        ax.plot(test_5m['timestamp'], pred_5m['lgbm'], label='LightGBM', \n",
    "                color='#2ECC71', linewidth=1, linestyle='-.', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Request Count')\n",
    "    ax.set_title('Model Predictions vs Actual (5-minute, Full Test Period)', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Zoomed (first 2 days)\n",
    "    ax = axes[1]\n",
    "    n_points = 288 * 2  # 2 days\n",
    "    ax.plot(test_5m['timestamp'].iloc[:n_points], pred_5m['actual'][:n_points], \n",
    "            label='Actual', color='black', linewidth=2)\n",
    "    \n",
    "    if 'prophet' in pred_5m:\n",
    "        ax.plot(test_5m['timestamp'].iloc[:n_points], pred_5m['prophet'][:n_points], \n",
    "                label='Prophet', color='#A23B72', linewidth=1.5, linestyle='--', alpha=0.8)\n",
    "    if 'sarima' in pred_5m:\n",
    "        ax.plot(test_5m['timestamp'].iloc[:n_points], pred_5m['sarima'][:n_points], \n",
    "                label='SARIMA', color='#F18F01', linewidth=1.5, linestyle=':', alpha=0.8)\n",
    "    if 'lgbm' in pred_5m:\n",
    "        ax.plot(test_5m['timestamp'].iloc[:n_points], pred_5m['lgbm'][:n_points], \n",
    "                label='LightGBM', color='#2ECC71', linewidth=1.5, linestyle='-.', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Request Count')\n",
    "    ax.set_title('Model Predictions vs Actual (5-minute, First 2 Days)', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_DIR / 'figures' / 'benchmark_predictions_5m.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best model per granularity\n",
    "print(\"\\nBest Model by Granularity (Lowest RMSE):\")\n",
    "print(\"-\" * 50)\n",
    "for gran in ['1m', '5m', '15m']:\n",
    "    subset = results_df[results_df['granularity'] == gran]\n",
    "    if len(subset) > 0:\n",
    "        best = subset.loc[subset['rmse'].idxmin()]\n",
    "        print(f\"  {gran}: {best['model']} (RMSE: {best['rmse']:.2f}, MAE: {best['mae']:.2f})\")\n",
    "\n",
    "# Overall best\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "overall_best = results_df.loc[results_df['rmse'].idxmin()]\n",
    "print(f\"\\nOVERALL BEST: {overall_best['model']} on {overall_best['granularity']}\")\n",
    "print(f\"  RMSE: {overall_best['rmse']:.2f}\")\n",
    "print(f\"  MAE:  {overall_best['mae']:.2f}\")\n",
    "print(f\"  MAPE: {overall_best['mape']:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Benchmark completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create formatted table for README\n",
    "print(\"\\n### Markdown Table for README\\n\")\n",
    "print(\"| Granularity | Model | RMSE | MAE | MAPE (%) |\")\n",
    "print(\"|-------------|-------|------|-----|----------|\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"| {row['granularity']} | {row['model']} | {row['rmse']:.2f} | {row['mae']:.2f} | {row['mape']:.2f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNotebook completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
